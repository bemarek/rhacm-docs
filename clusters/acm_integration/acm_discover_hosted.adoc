[#discovering-hcp-acm]
= Discovering {mce-short} hosted clusters in {product-title-short}

If you have {mce-short} clusters that are hosting multiple _hosted clusters_, you can bring those hosted clusters to a {product-title-short} hub cluster to manage with {product-title-short} management components, such as _Application lifecycle_ and _Governance_.

You can have those hosted clusters automatically discovered and imported as managed clusters.

*Note:* Since the hosted control planes run on the managed {mce-short} cluster nodes, the number of hosted control planes that the cluster can host is determined by the resource availability of managed {mce-short} cluster nodes, as well as the number of managed {mce-short} clusters.You can add more nodes or managed clusters to host more hosted control planes.

*Required access:* Cluster administrator

* <<discover-hosted-acm-prereqs, Prerequisites>>
* Configuring {product-title-short} to import {mce-short} clusters
* Importing {mce-short} 
* Discovering hosted clusters from MCE
* This requires `oc` and `clusteradm` CLIs.
//find this link

[#discover-hosted-acm-prereqs]
== Prerequisites

* You need one or more {mce-short} hosting clusters that are _managed_ clusters. These clusters are not {ocp} or {product-title-short} clusters because {mce-short} installs other operators, such as Hive and BareMetal infrastructure operators that you need.

* You need a {product-title-short} cluster set as your hub cluster.

[#config-acm-import-hosted]
== Configuring {product-title-short} to import hosted clusters

{mce-short} has a `local-cluster`, which is a hub cluster that is managed. The default add-ons are enabled for this `local-cluster`. Complete the following procedure to configure your local cluster to import:


. Run the following command to get the list of `managedclusteradd-ons` for your `local-cluster`:

+
[source,bash]
----
oc get managedclusteradd-ons -n local-cluster
----

See the example output of available add-ons:

+
[source,bash]
----
NAME                     AVAILABLE   DEGRADED   PROGRESSING
cluster-proxy            True                   False
hypershift-add-ons       True        False      False
managed-serviceaccount   True                   False
work-manager             True                   False
----

. Run the following command to get your add-ons:
//what are they doing here?

+
[source,bash]
----
oc get deployment -n open-cluster-management-agent-add-ons
----

See the following output:
//what are they looking for here?

+
[source,bash]
----
NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
cluster-proxy-proxy-agent              1/1     1            1         25h
hypershift-add-ons-agent               1/1     1            1         25h
klusterlet-add-ons-workmgr             1/1     1            1         25h
managed-serviceaccount-add-ons-agent   1/1     1            1         25h
----

[#config-add-ons-mce]
=== Configuring add-ons 

When your {mce-short} is imported into {product-title-short}, {product-title-short} enables the same set of add-ons to manage the {mce-short}. 

Install those add-ons in a different {mce-short} namespace so that the {mce-short} can self-manage with the `local-cluster` add-ons while  {product-title-short} manages {mce-short} at the same time. Complete the following procedure:

. Log in to your {product-title-short} with the CLI.

. Create the `add-onsDeploymentConfig` resource to specify a different add-on installation namespace. See the following example where `agentInstallNamespace` points to `open-cluster-management-agent-add-ons-discovery`:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: add-onsDeploymentConfig
metadata:
  name: add-ons-ns-config
  namespace: multicluster-engine
spec:
  agentInstallNamespace: open-cluster-management-agent-add-ons-discovery
----

. Update the existing `ClusterManagementadd-ons` resources for the add-ons so that the `add-onsDeploymentConfig` resource have the `agentInstallNamespace` namespace that you created.
//is this the highlevel step and adding work manager is part 1? this is simlar to the step after it.

See the following `ClusterManagementadd-ons` example resource for the `work-manager` add-on:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: work-manager
spec:
  add-onsMeta:
    displayName: work-manager
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
    type: Placements
----

. Add the `add-onsDeploymentConfig` to the `ClusterManagementadd-ons`. See the following example:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: work-manager
spec:
  add-onsMeta:
    displayName: work-manager
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
      configs:
      - group: add-ons.open-cluster-management.io
        name: add-ons-ns-config
        namespace: multicluster-engine
        resource: add-onsdeploymentconfigs
    type: Placements
----

. Add the `add-onsDeploymentConfig` to the `managed-serviceaccount`. See the following example:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: managed-serviceaccount
spec:
  add-onsMeta:
    displayName: managed-serviceaccount
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
      configs:
      - group: add-ons.open-cluster-management.io
        name: add-ons-ns-config
        namespace: multicluster-engine
        resource: add-onsdeploymentconfigs
    type: Placements
----

. Add the `add-onsDeploymentConfig` to the `cluster-proxy` add-ons. See the following example:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: cluster-proxy
spec:
  add-onsMeta:
    displayName: cluster-proxy
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
      configs:
      - group: add-ons.open-cluster-management.io
        name: add-ons-ns-config
        namespace: multicluster-engine
        resource: add-onsdeploymentconfigs
    type: Placements
----

//we need to apply the file.

The add-ons for the {product-title-short} `local-cluster` and all other managed clusters are re-installed into the namespace that you specified. 

Run the following command to verify:
//??

+
[source,bash]
----
oc get deployment -n open-cluster-management-agent-add-ons-discovery
----

. See the following output:

+
[source,bash]
----
NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
cluster-proxy-proxy-agent            1/1     1            1           24h
klusterlet-add-ons-workmgr             1/1     1            1           24h
managed-serviceaccount-add-ons-agent   1/1     1            1           24h
----

[#create-klusterletconfig]
=== Creating a _KlusterletConfig_ resource

When a `ManagedCluster` references the `KlusterletConfig` resource, the managed cluster `klusterlet` is installed in the namespace that you specified in the `KlusterletConfig`. 

Create a `KlusterletConfig` resource that is used by `ManagedCluster` resources to import {mce-short} clusters. 

You can import the {product-title-short} klusterlet to be installed in a different namespace than the {mce-short} klusterlet for the `local-cluster` in the {mce-short} cluster.
//still struggling with this

. Create a `KlusterletConfig` using the following example:
//?

+
[source,yaml]
----
kind: KlusterletConfig
apiVersion: config.open-cluster-management.io/v1alpha1
metadata:
  name: mce-import-klusterlet-config
spec:
  installMode:
    type: noOperator
    noOperator:
       postfix: mce-import
----
//apply?

[#backup-restore-discover]
=== Configure for backup and restore

Since you installed {product-title-short} , you can also use the _Backup and restore_ feature.

If the hub cluster is restored in a disaster recovery scenario, the imported {mce-short} clusters and hosted clusters are imported to the newer {product-title-short} hub cluster. 

In this scenario, you need to restore the previous configurations as part of {product-title-short} hub cluster restore. 

Add `backup=true` to enable backup. See the following steps for each add-on:

* For your `add-ons-ns-config`, run the following command:

+
[source,bash]
----
oc label add-onsdeploymentconfig add-ons-ns-config -n multicluster-engine cluster.open-cluster-management.io/backup=true
----

* For your `hypershift-add-ons-deploy-config`, run the following command:

+
[source,bash]
----
oc label add-onsdeploymentconfig hypershift-add-ons-deploy-config -n multicluster-engine cluster.open-cluster-management.io/backup=true
----

* For your `work-manager`, run the following command:

+
[source,bash]
----
oc label clustermanagementadd-ons work-manager cluster.open-cluster-management.io/backup=true
----

* For your `cluster-proxy `, run the following command:

+
[source,bash]
----
oc label clustermanagementadd-ons cluster-proxy cluster.open-cluster-management.io/backup=true
----

* For your `managed-serviceaccount`, run the following command:

+
[source,bash]
----
oc label clustermanagementadd-ons managed-serviceaccount cluster.open-cluster-management.io/backup=true
----

* For your `mce-import-klusterlet-config`, run the following command:

+
[source,bash]
----
oc label KlusterletConfig mce-import-klusterlet-config cluster.open-cluster-management.io/backup=true
----

[#import]
== Importing {mce-short} manually

. From your {product-title-short} cluster, create a `ManagedCluster` resource manually to import an {mce-short} cluster. 

+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  annotations:
    agent.open-cluster-management.io/klusterlet-config: mce-import-klusterlet-config <1>
  name: mce-a <2>
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60
----

1. The `mce-import-klusterlet-config` annotation references the `KlusterletConfig` resource that you created in the previous step to install the {product-title-short} klusterlet into a different namespace in {mce-short}.
2. The example imports an {mce-short} managed cluster named `mce-a`.

The managed cluster and the namespace is created in the {product-title-short} cluster. 
//verify
????

. Add the auto import secret. Follow https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.10/html-single/clusters/index#importing-clusters-auto-import-secret to create the auto-import secret to complete the {mce-short} auto-import process. 

After you create the auto import secret in the {mce-short} managed cluster namespace in the {product-title-short} cluster, the managed cluster gets registered.

Run the following command to get the status:

+
[source,bash]
----
oc get managedcluster
----

See following output with the status of managed clusters:
+
[source,bash]
----
NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                                         JOINED   AVAILABLE   AGE
local-cluster   true           https://api.acm-hub-hs-aws.dev09.red-chesterfield.com:6443   True     True        44h
mce-a           true           https://api.clc-hs-mce-a.dev09.red-chesterfield.com:6443     True     True        27s
----

*Important:* Do not enable any other {product-title-short} add-ons for the imported {mce-short}.

== Discovering hosted clusters

After all {mce-short} clusters are imported into {product-title-short}, you need to enable the hypershift add-on for those managed {mce-short} clusters to discover the hosted clusters.

in the {product-title-short} hub cluster to enable the hypershift add-on. Similar to how the default add-ons are intalled into a different namespace in the previous section, these commands are for installing the hypershift add-ons into a different namespace in {mce-short} as well so that the hypershift add-ons agent for {mce-short}'s local-cluster and the agent for {product-title-short} can co-exist in {mce-short}. 

. Run the following commands to set the `agentInstallNamespace` namespace of the add-on to `open-cluster-management-agent-add-ons-discovery`:
----
oc patch add-onsdeploymentconfig hypershift-add-ons-deploy-config -n multicluster-engine --type=merge -p '{"spec":{"agentInstallNamespace":"open-cluster-management-agent-add-ons-discovery"}}'
----

. Run the following commands to disable metrics and to disable the hypershift operator management:

----
oc patch add-onsdeploymentconfig hypershift-add-ons-deploy-config -n multicluster-engine --type=merge -p '{"spec":{"customizedVariables":[{"name":"disableMetrics","value": "true"},{"name":"disableHOManagement","value": "true"}]}}'
----
. Run the following commands to enable the hypershift add-on for {mce-short}:

----
clusteradm add-ons enable --names hypershift-addon --clusters <managed cluster names>
----

Replace <managed cluster names> with the actual managed cluster names for {mce-short}, comma separated. 

. You can get the {mce-short} managed cluster names by running the following command in {product-title-short}.

----
oc get managedcluster
----

. Log into {mce-short} clusters and verify that the hypershift add-on is installed in the specified namespace. Run the following command:

----
oc get deployment -n open-cluster-management-agent-add-ons-discovery
----

----
NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
klusterlet-add-ons-workmgr             1/1     1            1           24h
hypershift-add-ons-agent               1/1     1            1           24h
managed-serviceaccount-add-ons-agent   1/1     1            1           24h
----

This hypershift add-on is deployed by {product-title-short} acts as a discovery agent that discovers hosted clusters from {mce-short} and create corresponding `DiscoveredCluster` CR in the {mce-short}'s managed cluster namespace in the {product-title-short} hub cluster when the hosted cluster's kube API server becomes available. 

Log into {product-title-short} hub console, navigate to *All Clusters* > *Infrastructure* > *Clusters*. Find the _Discovered clusters_ tab to view all discovered hosted clusters from {mce-short} with type `MultiClusterEngineHCP`. 


Next go to the HCP Import (new file)
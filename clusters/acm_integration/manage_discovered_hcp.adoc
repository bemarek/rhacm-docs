[#discovering-hcp-acm]
= Discovering and managing hosted clusters 

If you have {mce-short} clusters that are hosting many hosted clusters, you can bring these hosted clusters to a {product-title-short} hub cluster for management.

By using {product-title-short} management components, such a _Application lifecycle_ and _Governance_, you can import existing {mce-short} clusters to a {product-title-short} hub cluster. 
//local cluster?

You can have those hosted clusters automatically discovered and imported as managed clusters.
//check this order of paragraphs

* <<-manage-discover-prereqs,Prerequisites>>

[#manage-discover-prereqs]
== Prerequisites

* You need one or more {mce-short} hosting clusters that are _managed_ clusters. These clusters are not {ocp} or {product-title-short} clusters because {mce-short} installs other operators, such as Hive and BareMetal infrastructure operators that you need.

* Install {product-title-short}. See the {product-title-short} link:../../install/install_overview.adoc#installing[Installing and upgrading] documentation.

* You need a {product-title-short} cluster as your hub cluster.

[#manage-discover-scale]
== Scaling
//title and location

Since the hosted control planes run on the managed {mce-short} cluster nodes, the number of hosted control planes the cluster can host is determined by the resource availability of managed {mce-short} cluster nodes, as well as the number of managed {mce-short} clusters. 

You can add more nodes or managed clusters to host more hosted control planes.

Importing an {mce-short} cluster into {product-title-short}

[#manage-discover-scale]
== Configuring for import
//check this order of tasks if we break out this in to two files: Disover/Manage and Auto import

{mce-short} has a self-managed cluster called `local-cluster` and the default add-ons are enabled for this managed cluster.
//we  define this as hub that is managed in the doc

//what are we asking them to do here? Double check my work:

. Run the following command to get the `managedclusteradd-ons` status for your `local-cluster`:

+
[source,bash]
----
oc get managedclusteradd-ons -n local-cluster
----

See the example output:

+
[source,bash]
----
NAME                     AVAILABLE   DEGRADED   PROGRESSING
cluster-proxy            True                   False
hypershift-add-ons         True        False      False
managed-serviceaccount   True                   False
work-manager             True                   False
----

. Run the following command to 
//what are they doing here?

+
[source,bash]
----
oc get deployment -n open-cluster-management-agent-add-ons
----

See the following output...:
//what are they looking for here?

+
[source,bash]
----
NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
cluster-proxy-proxy-agent            1/1     1            1           25h
hypershift-add-ons-agent               1/1     1            1           25h
klusterlet-add-ons-workmgr             1/1     1            1           25h
managed-serviceaccount-add-ons-agent   1/1     1            1           25h
----

[#install-add-onss-mce]
=== Installing add-ons 
// Check the order here of the tasks, has import happened?

When this {mce-short} is imported into {product-title-short}, {product-title-short} enables the same set of add-ons to manage the {mce-short}. 

Install those add-ons in a different namespace in {mce-short} so that {mce-short} can still self-manage with the `local-cluster` add-ons while {mce-short} can be managed by {product-title-short} at the same time.

. Log in to your {product-title-short} with the CLI.
//hub cluster? 

. Create the `add-onsDeploymentConfig` resource to specify a different add-on installation namespace. See the following example where `agentInstallNamespace` points to `open-cluster-management-agent-add-ons-discovery`:
//check what we want to highlight here?

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: add-onsDeploymentConfig
metadata:
  name: add-ons-ns-config
  namespace: multicluster-engine
spec:
  agentInstallNamespace: open-cluster-management-agent-add-ons-discovery
----

. Update the existing `ClusterManagementadd-ons` resources for these add-ons so that the add-ons have the `agentInstallNamespace` namespace that you created in the  `add-onsDeploymentConfig` resource.

See the following `ClusterManagementadd-ons` example resource for the `work-manager` add-on:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: work-manager
spec:
  add-onsMeta:
    displayName: work-manager
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
    type: Placements
----

. Add the `add-onsDeploymentConfig` to the `ClusterManagementadd-ons`. See the following example:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: work-manager
spec:
  add-onsMeta:
    displayName: work-manager
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
      configs:
      - group: add-ons.open-cluster-management.io
        name: add-ons-ns-config
        namespace: multicluster-engine
        resource: add-onsdeploymentconfigs
    type: Placements
----

. Add the `add-onsDeploymentConfig` to the `managed-serviceaccount`. See the following example:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: managed-serviceaccount
spec:
  add-onsMeta:
    displayName: managed-serviceaccount
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
      configs:
      - group: add-ons.open-cluster-management.io
        name: add-ons-ns-config
        namespace: multicluster-engine
        resource: add-onsdeploymentconfigs
    type: Placements
----

. Add the `add-onsDeploymentConfig` to the `cluster-proxy` add-ons. See the following example:

+
[source,yaml]
----
apiVersion: add-ons.open-cluster-management.io/v1alpha1
kind: ClusterManagementadd-ons
metadata:
  name: cluster-proxy
spec:
  add-onsMeta:
    displayName: cluster-proxy
  installStrategy:
    placements:
    - name: global
      namespace: open-cluster-management-global-set
      rolloutStrategy:
        type: All
      configs:
      - group: add-ons.open-cluster-management.io
        name: add-ons-ns-config
        namespace: multicluster-engine
        resource: add-onsdeploymentconfigs
    type: Placements
----

The add-ons for the {product-title-short} `local-cluster` and all other managed clusters are re-installed into the namespace that you specified. 
//way to verify?

//Not sure what this is -- to check??:
+
[source,bash]
----
oc get deployment -n open-cluster-management-agent-add-ons-discovery
----

. See the following output:

+
[source,bash]
----
NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
cluster-proxy-proxy-agent            1/1     1            1           24h
klusterlet-add-ons-workmgr             1/1     1            1           24h
managed-serviceaccount-add-ons-agent   1/1     1            1           24h
----

[#create-klusterletconfig]
=== Creating a _KlusterletConfig_ resource

You need to create a `KlusterletConfig` resource that is used by `ManagedCluster` resources to import {mce-short} clusters. 

When a `ManagedCluster` references this `KlusterletConfig` resource, the managed cluster klusterlet is installed in the namespace that you specified in the `KlusterletConfig`. 

You can import the {product-title-short} klusterlet to be installed in a different namespace than the {mce-short} klusterlet for the self-managed `local-cluster`` managed cluster in the {mce-short} cluster.
//fix this--this is a bit hard to understand, and use steps to lead to the YAML:

+
[source,yaml]
----
kind: KlusterletConfig
apiVersion: config.open-cluster-management.io/v1alpha1
metadata:
  name: mce-import-klusterlet-config
spec:
  installMode:
    type: noOperator
    noOperator:
       postfix: mce-import
----

[#backup-restore-discover]
=== Configure for backup and restore
//location of this section?Needs to be a task?

Since you have {product-title-short} installed, you can use the _Backup and restore_ feature.

If the hub cluster is restored in a disaster recovery scenario, the imported {mce-short} clusters and the hosted clusters are imported to the newer {product-title-short} hub cluster. 

The previous configurations need to be restored as part of {product-title-short} hub cluster restore. 

Add `backup=true`  to those resources to enable backup. See the following commands:

. For your `add-ons-ns-config`, run the following command:

+
[source,bash]
----
oc label add-onsdeploymentconfig add-ons-ns-config -n multicluster-engine cluster.open-cluster-management.io/backup=true
----

. For your `hypershift-add-ons-deploy-config`, run the following command:

+
[source,bash]
----
oc label add-onsdeploymentconfig hypershift-add-ons-deploy-config -n multicluster-engine cluster.open-cluster-management.io/backup=true
----

. For your `work-manager`, run the following command:

+
[source,bash]
----
oc label clustermanagementadd-ons work-manager cluster.open-cluster-management.io/backup=true
----

. For your `cluster-proxy `, run the following command:

+
[source,bash]
----
oc label clustermanagementadd-ons cluster-proxy cluster.open-cluster-management.io/backup=true
----

. For your `managed-serviceaccount`, run the following command:
+
[source,bash]
----
oc label clustermanagementadd-ons managed-serviceaccount cluster.open-cluster-management.io/backup=true
----

. For your `mce-import-klusterlet-config`, run the following command:

+
[source,bash]
----
oc label KlusterletConfig mce-import-klusterlet-config cluster.open-cluster-management.io/backup=true
----

[#import]
== Importing {mce-short} 

. From your {product-title-short} cluster, create a `ManagedCluster` resource manually to import an {mce-short} cluster. See the following resource to import an {mce-short} managed cluster named `mce-a`.

+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  annotations:
    agent.open-cluster-management.io/klusterlet-config: mce-import-klusterlet-config
  name: mce-a
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60
----

*Note:* See the  `agent.open-cluster-management.io/klusterlet-config: mce-import-klusterlet-config` annotation that references the `KlusterletConfig` resource that you created in the previous step to install the {product-title-short} klusterlet into a different namespace in {mce-short}.
//this is a mouthful

The managed cluster and the namespace is created in the {product-title-short} cluster. 
//verify?

Follow https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.10/html-single/clusters/index#importing-clusters-auto-import-secret to create the auto-import secret to complete the {mce-short} auto-import process. Once the auto import secret is created in the {mce-short} managed cluster namespace in the {product-title-short} cluster, the managed cluster gets registered and you should see the managed cluster status like this.
//This is a step.


----
oc get managedcluster
NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                                         JOINED   AVAILABLE   AGE
local-cluster   true           https://api.acm-hub-hs-aws.dev09.red-chesterfield.com:6443   True     True        44h
mce-a           true           https://api.clc-hs-mce-a.dev09.red-chesterfield.com:6443     True     True        27s
----

Important: DO NOT enable any other {product-title-short} add-ons for the imported {mce-short}.

## Enabling the hypershift add-ons for {mce-short}

After all {mce-short} clusters are imported into {product-title-short}, you need to enable the hypershift add-ons for those managed {mce-short} clusters. Run the following commands in the {product-title-short} hub cluster to enable it. Similar to how the default add-ons are intalled into a different namespace in the previous section, these commands are for installing the hypershift add-ons into a different namespace in {mce-short} as well so that the hypershift add-ons agent for {mce-short}'s local-cluster and the agent for {product-title-short} can co-exist in {mce-short}. This requires `oc` and `clusteradm` CLIs.
//fix

----
oc patch add-onsdeploymentconfig hypershift-add-ons-deploy-config -n multicluster-engine --type=merge -p '{"spec":{"agentInstallNamespace":"open-cluster-management-agent-add-ons-discovery"}}'

oc patch add-onsdeploymentconfig hypershift-add-ons-deploy-config -n multicluster-engine --type=merge -p '{"spec":{"customizedVariables":[{"name":"disableMetrics","value": "true"},{"name":"disableHOManagement","value": "true"}]}}'

% clusteradm add-ons enable --names hypershift-add-ons --clusters <{mce-short} managed cluster names>
----

Replace <{mce-short} managed cluster names> with the actual managed cluster names for {mce-short}, comma separated. You can get the {mce-short} managed cluster names by running the following command in {product-title-short}.

----
oc get managedcluster
----

Log into {mce-short} clusters and verify that the hypershift add-ons is installed in the specified namespace.

----
oc get deployment -n open-cluster-management-agent-add-ons-discovery
NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
klusterlet-add-ons-workmgr             1/1     1            1           24h
hypershift-add-ons-agent               1/1     1            1           24h
managed-serviceaccount-add-ons-agent   1/1     1            1           24h
----

This hypershift add-ons deployed by {product-title-short} acts as a discovery agent that discovers hosted clusters from {mce-short} and create corresponding `DiscoveredCluster` CR in the {mce-short}'s managed cluster namespace in the {product-title-short} hub cluster when the hosted cluster's kube API server becomes available. Log into {product-title-short} hub console, navigate to All Clusters -> Infrastructure -> Clusters and `Discovered clusters` tab to view all discovered hosted clusters from {mce-short} with type `MultiClusterEngineHCP`. 

[#import-discover-hcp]
== Automating import for discovered hosted clusters
//I think this needs to be a separate file like the ROSA file
//this will be built similar to: https://github.com/stolostron/rhacm-docs/pull/6608/files

Automate the import of hosted clusters by using the `DiscoveredCluster` resource for faster cluster management, without manually importing individual clusters.

When a discovered hosted cluster is auto-imported into {product-title-short}, all {product-title-short} add-ons are enabled as well so you can start managing the hosted clusters using the available management tools.

The hosted cluster is also auto-imported into {mce-short}. Through the {mce-short} console, you can manage the hosted cluster's life-cycle. You cannot manage the hosted cluster life-cycle from the {product-title-short} console.

*Required access:* Cluster administrator

[#autoimport-hcp]
== Prerequisites

. Install {product-title-short}. See the {product-title-short} link:../../install/install_overview.adoc#installing[Installing and upgrading] documentation.
//check

[#importing-disc-hcp]
== Importing
//work on this title

The following procedure is an example of how to import all your discovered hosted clusters automatically. 
 
Log in to your hub cluster from the CLI to complete the following procedure:

. Create a YAML file with the following example and apply the changes that are referenced:

A `DiscoveredCluster` CR that is created by {product-title-short}'s hypershift add-ons agent looks like this.

+
[source,yaml]
----
apiVersion: discovery.open-cluster-management.io/v1
kind: DiscoveredCluster
metadata:
  creationTimestamp: "2024-05-30T23:05:39Z"
  generation: 1
  labels:
    hypershift.open-cluster-management.io/hc-name: hosted-cluster-1
    hypershift.open-cluster-management.io/hc-namespace: clusters
  name: hosted-cluster-1
  namespace: mce-1
  resourceVersion: "1740725"
  uid: b4c36dca-a0c4-49f9-9673-f561e601d837
spec:
  apiUrl: https://a43e6fe6dcef244f8b72c30426fb6ae3-ea3fec7b113c88da.elb.us-west-1.amazonaws.com:6443
  cloudProvider: aws
  creationTimestamp: "2024-05-30T23:02:45Z"
  credential: {}
  displayName: mce-1-hosted-cluster-1
  importAsManagedCluster: false
  isManagedCluster: false
  name: hosted-cluster-1
  openshiftVersion: 0.0.0
  status: Active
  type: MultiClusterEngineHCP
----

Setting the `spec.importAsManagedCluster` to `true` triggers {product-title-short}'s discovery operator to start the auto-importing process and soon, you will see a managed cluster that is named the same as `spec.displayName` in the `DiscoveredCluster`. 

Setting `spec.importAsManagedCluster` to `true` can be automated by applying the following policy to {product-title-short}. This policy ensures that a DiscoveredCluster with type `MultiClusterEngineHCP` is set for auto-importing.

[#creating-rosa-policy]
== Creating the automatic import policy

The following policy and procedure is an example of how to import all your discovered hosted clusters automatically. 
 
Log in to your hub cluster from the CLI to complete the following procedure:

. Create a YAML file with the following example and apply the changes that are referenced:

+
[source,yaml] 
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-mce-hcp-autoimport
  namespace: open-cluster-management-global-set
  annotations:
    policy.open-cluster-management.io/standards: NIST SP 800-53
    policy.open-cluster-management.io/categories: CM Configuration Management
    policy.open-cluster-management.io/controls: CM-2 Baseline Configuration
    policy.open-cluster-management.io/description: Discovered clusters that are of
      type MultiClusterEngineHCP can be automatically imported into {product-title-short} as managed clusters.
      This policy configure those discovered clusters so they are automatically imported. 
      Fine tuning MultiClusterEngineHCP clusters to be automatically imported
      can be done by configure filters at the configMap or add annotation to the discoverd cluster.
spec:
  # Remove the default remediation below to enforce the policies.
  # remediationAction: inform
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: mce-hcp-autoimport-config
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: v1
                kind: ConfigMap
                metadata:
                  name: discovery-config
                  namespace: open-cluster-management-global-set
                data:
                  rosa-filter: ""
          remediationAction: enforce
          severity: low
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-mce-hcp-autoimport
        spec:
          remediationAction: enforce
          severity: low
          object-templates-raw: |
            {{- /* find the MultiClusterEngineHCP DiscoveredClusters */ -}}
            {{- range $dc := (lookup "discovery.open-cluster-management.io/v1" "DiscoveredCluster" "" "").items }}
              {{- /* Check for the flag that indicates the import should be skipped */ -}}
              {{- $skip := "false" -}}
              {{- range $key, $value := $dc.metadata.annotations }}
                {{- if and (eq $key "discovery.open-cluster-management.io/previously-auto-imported")
                           (eq $value "true") }}
                  {{- $skip = "true" }}
                {{- end }}
              {{- end }}
              {{- /* if the type is MultiClusterEngineHCP and the status is Active */ -}}
              {{- if and (eq $dc.spec.status "Active") 
                         (contains (fromConfigMap "open-cluster-management-global-set" "discovery-config" "mce-hcp-filter") $dc.spec.displayName)
                         (eq $dc.spec.type "MultiClusterEngineHCP")
                         (eq $skip "false") }}
            - complianceType: musthave
              objectDefinition:
                apiVersion: discovery.open-cluster-management.io/v1
                kind: DiscoveredCluster
                metadata:
                  name: {{ $dc.metadata.name }}
                  namespace: {{ $dc.metadata.namespace }}
                spec:
                  importAsManagedCluster: true
              {{- end }}
            {{- end }}
----
//we need to pull the steps and call outs here, see the rosa file for how I set this up from a large YAML file with no steps in the draft.

[#create-hcp-placement]
== Creating the placement definition 

You need to create a placement definition that specifies the managed cluster for the policy deployment.

. Create the placement definition that selects only the `local-cluster`, which is a hub cluster that is managed. Use the following YAML sample:

+
[source,yaml] 
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: policy-mce-hcp-autoimport-placement
  namespace: open-cluster-management-global-set
spec:
  tolerations:
    - key: cluster.open-cluster-management.io/unreachable
      operator: Exists
    - key: cluster.open-cluster-management.io/unavailable
      operator: Exists
  clusterSets:
    - global
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchExpressions:
            - key: local-cluster
              operator: In
              values:
                - "true"
----

. Run `oc apply -f placement.yaml -n <namespace>`, where `namespace` matches the namespace that you used for the policy that you previously created. 

[#bind-hcp-placement]
== Binding the import policy to a placement definition

After you create the policy and the placement, you need to connect the two resources.

. Connect the resources by using a `PlacementBinding`. See the following example where `placementRef` points to the `Placement` that you created, and `subjects` points to the `Policy` that you created:

+
[source,yaml]
----
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: policy-mce-hcp-autoimport-placement-binding
  namespace: open-cluster-management-global-set
placementRef:
  name: policy-mce-hcp-autoimport-placement
  apiGroup: cluster.open-cluster-management.io
  kind: Placement
subjects:
  - name: policy-mce-hcp-autoimport
    apiGroup: policy.open-cluster-management.io
    kind: Policy
----
//work live with roke, I set this up like the rosa file

. To verify, run the following command:

+
----
oc get policy policy-mce-hcp-autoimport -n <namespace>
---- 

## Detaching hosted clusters from {product-title-short}

An imported hosted cluster can be detached from {product-title-short} using the detach option in the {product-title-short} console or by removing the corresponsing `ManagedCluster` CR from the command line. It is recommended to detach the managed hosted cluster before destroying the hosted cluster.

When a discovered cluster is detached, the following annotation is added to the DiscoveredCluster resource to prevent the policy to import the discovered cluster again.

----
  annotations:
    discovery.open-cluster-management.io/previously-auto-imported: "true"
----

If you want the detached discovered cluster to be re-imported, this annotation needs to be remove

## Limitations
//these should either be in limitations or weaved into the doc

- The discovered cluster name link on the discovered cluster list UI does not open the console for discovered cluster with `MultiClusterEngineHCP` type.

<img width="1098" alt="image" src="https://github.com/rokej/hypershift-add-ons-operator/assets/41969005/ae9efc82-f4b2-462c-862f-0da62e8f1b87">

- The "Import cluster" discovered cluster action menu option should not be used to import `MultiClusterEngineHCP` type discovered clusters. The only way to import them is through the auto-import policy.

<img width="1138" alt="image" src="https://github.com/rokej/hypershift-add-ons-operator/assets/41969005/a86a0f73-04e0-4a89-a355-43c15565ef66">

- The "Last active" column for `MultiClusterEngineHCP` type discovered clusters is always "N/A".